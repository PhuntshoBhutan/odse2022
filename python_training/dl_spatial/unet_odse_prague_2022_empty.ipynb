{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh-MgyQM2Qw9"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The U-Net model is a simple fully  convolutional neural network that is used for binary segmentation i.e foreground and background pixel-wise classification. Mainly, it consists of two parts. \n",
        "\n",
        "*   Encoder: we apply a series of conv layers and downsampling layers  (max-pooling) layers to reduce the spatial size \n",
        "*   Decoder: we apply a series of upsampling layers to reconstruct the spatial size of the input. \n",
        "\n",
        "The two parts are connected using a concatenation layers among different levels. This allows learning different features at different levels. At the end we have a simple conv 1x1 layer to reduce the number of channels to 1.\n",
        "\n",
        "Find the original paper [here](https://arxiv.org/abs/1505.04597)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1eKe1hMcn_kRc1xaakIYkFmHlmi9SKUbR)"
      ],
      "metadata": {
        "id": "wt27rHMPNIgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is full of holes - it is intended to be filled by a user as a hands-on practice. To see one of possible versions with outputs, see <https://colab.research.google.com/drive/1tuTJ6vzNzsacebWkK-OH7WvcHoh3k8Hb>. \n",
        "\n",
        "Important - do a copy of that notebook and modify your own copy, otherwise you destroy it for all the other users."
      ],
      "metadata": {
        "id": "E-KmST1VLgQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and organize the training dataset"
      ],
      "metadata": {
        "id": "RkVTDBCVWHRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is going to be downloaded using `odse_dl` Python package."
      ],
      "metadata": {
        "id": "uTlKlMBbWVZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install `odse_dl` requirements"
      ],
      "metadata": {
        "id": "B7nP_VEsWgMg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgbHKw1rdDm_"
      },
      "outputs": [],
      "source": [
        "!pip install -q geopandas rasterio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install `odse_dl`"
      ],
      "metadata": {
        "id": "0aboWJNaWq6y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU-T5-3Rb44i"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"git+https://gitlab.com/geoharmonizer_inea/odse-workshop-2022.git#subdirectory=python_training/packages/odse_dl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOpRsS-TzhRg"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbOVYwZ5cs_8"
      },
      "outputs": [],
      "source": [
        "from odse_dl import data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check what data can the package download"
      ],
      "metadata": {
        "id": "1LljZM3pYQkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.sentinel_urls"
      ],
      "metadata": {
        "id": "u4ZSGhq_YKy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6knItmkgcucT"
      },
      "outputs": [],
      "source": [
        "# according to the documentation, we can download them with the following:\n",
        "# files = data.get_sentinel_tiles()\n",
        "\n",
        "# but hey, they are soooo many. I'm gonna download only the summertime ones for this demo:\n",
        "urls = [*filter(lambda url: '06.25..' in url and any(band in url for band in ('red', 'green', 'blue', 'nir')), data.sentinel_urls)]\n",
        "files = data.input_data_to_tiles(urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's organize them in some nice order. E.g.:\n",
        "\n",
        "```\n",
        "path/to/my/dataset/\n",
        "├── train_images\n",
        "│   ├── image_0.tif\n",
        "│   ├── image_1.tif\n",
        "│   ├── image_2.tif\n",
        "│   └── image_4.tif\n",
        "├── train_masks\n",
        "│   ├── image_0.tif\n",
        "│   ├── image_1.tif\n",
        "│   ├── image_2.tif\n",
        "│   └── image_4.tif\n",
        "├── val_images\n",
        "│   └── image_3.tif\n",
        "└── val_masks\n",
        "    └── image_3.tif\n",
        "```"
      ],
      "metadata": {
        "id": "0oeMnYT7Yu7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax8UVLjt2wS5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dir_paths = ('training_set', 'training_set/train_images', \n",
        "             'training_set/train_masks', 'training_set/train_masks', \n",
        "             'training_set/val_images', 'training_set/val_masks')\n",
        "\n",
        "for dir_path in dir_paths:\n",
        "    if not os.path.isdir(dir_path):\n",
        "        os.mkdir(dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's copy the labels from `odse_dl.data` and make stacks of individual bands with `GDAL`"
      ],
      "metadata": {
        "id": "6kz4IlUtZtDv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUu2zy9u4t7Q"
      },
      "outputs": [],
      "source": [
        "# copy labels (target data)\n",
        "!for i in 1 3; do cp /usr/local/lib/python3.7/dist-packages/odse_dl/dist_data/target_t${i}.tif training_set/train_masks/t${i}.tif; done\n",
        "!for i in 4; do cp /usr/local/lib/python3.7/dist-packages/odse_dl/dist_data/target_t${i}.tif training_set/val_masks/t${i}.tif; done\n",
        "\n",
        "# create stacks of Sentinel images (using here only RGB + NIR, but feel free to be more brave)\n",
        "!for i in 1 3; do gdal_merge.py -separate  -o training_set/train_images/t${i}.tif -co PHOTOMETRIC=MINISBLACK input_data/lcv_red_sentinel.s2l2a_p50_30m_0..0cm_2018.06.25..2018.09.12_eumap_epsg3035_v1.0_t${i}.tif input_data/lcv_green_sentinel.s2l2a_p50_30m_0..0cm_2018.06.25..2018.09.12_eumap_epsg3035_v1.0_t${i}.tif input_data/lcv_blue_sentinel.s2l2a_p50_30m_0..0cm_2018.06.25..2018.09.12_eumap_epsg3035_v1.0_t${i}.tif input_data/lcv_nir_sentinel.s2l2a_p50_30m_0..0cm_2018.06.25..2018.09.12_eumap_epsg3035_v1.0_t${i}.tif; done\n",
        "!for i in 4; do gdal_merge.py -separate  -o training_set/val_images/t${i}.tif -co PHOTOMETRIC=MINISBLACK input_data/lcv_red_sentinel.s2l2a_p50_30m_0..0cm_2018.06.25..2018.09.12_eumap_epsg3035_v1.0_t${i}.tif input_data/lcv_green_sentinel.s2l2a_p50_30m_0..0cm_2018.06.25..2018.09.12_eumap_epsg3035_v1.0_t${i}.tif input_data/lcv_blue_sentinel.s2l2a_p50_30m_0..0cm_2018.06.25..2018.09.12_eumap_epsg3035_v1.0_t${i}.tif input_data/lcv_nir_sentinel.s2l2a_p50_30m_0..0cm_2018.06.25..2018.09.12_eumap_epsg3035_v1.0_t${i}.tif; done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LisvDshzevQ"
      },
      "source": [
        "# Let's write the utils we are gonna need for the model itself"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import a million of packages and modules we are going to use."
      ],
      "metadata": {
        "id": "qJA1czCUV_eX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdeDl5HO0QsY"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from odse_dl import legend\n",
        "from osgeo import gdal\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Concatenate, UpSampling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generators"
      ],
      "metadata": {
        "id": "uBJnTvbRjCGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a data generator - a [generator](https://wiki.python.org/moin/Generators) object yielding image data from a specified directory in the needed form (a batch of `numpy` arrays). [Onehot encoding](https://www.kaggle.com/code/dansbecker/using-categorical-data-with-one-hot-encoding/notebook) is performed to help the training accuracy."
      ],
      "metadata": {
        "id": "bZdestSafu5x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0nw_jHI2whf"
      },
      "outputs": [],
      "source": [
        "class AugmentGenerator:\n",
        "    \"\"\"Data generator.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, batch_size=5, operation='train',\n",
        "                 onehot_encode=True):\n",
        "        \"\"\"Initialize the generator.\n",
        "\n",
        "        :param data_dir: path to the directory containing images\n",
        "        :param batch_size: the number of samples that will be propagated\n",
        "            through the network at once\n",
        "        :param operation: either 'train' or 'val'\n",
        "        :param onehot_encode: boolean to onehot-encode masks during training\n",
        "        \"\"\"\n",
        "        images_dir = os.path.join(\n",
        "            data_dir, '{}_images'.format(operation))\n",
        "        masks_dir = os.path.join(\n",
        "            data_dir, '{}_masks'.format(operation))\n",
        "\n",
        "        # create variables useful throughout the entire class\n",
        "        self.batch_size = batch_size\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.perform_onehot_encoding = onehot_encode\n",
        "\n",
        "    def __call__(self):\n",
        "        \"\"\"Generate batches of data.\n",
        "\n",
        "        :return: yielded tuple of batch-sized np stacks of validation images\n",
        "            and masks\n",
        "        \"\"\"\n",
        "        return self.generate_numpy()\n",
        "\n",
        "    def generate_numpy(self):\n",
        "        \"\"\"Generate batches of data using our own numpy generator.\n",
        "\n",
        "        Note: tf.data.Dataset.from_generator() seemed to be useful and maybe\n",
        "        could speed up the process little bit , but it seemed not to work\n",
        "        properly when __call__ takes arguments.\n",
        "\n",
        "        :return: yielded tuple of batch-sized np stacks of validation images\n",
        "            and masks\n",
        "        \"\"\"\n",
        "        # create generators\n",
        "        image_generator = self.numpy_generator(\n",
        "            self.images_dir, self.batch_size)\n",
        "        mask_generator = self.numpy_generator(\n",
        "            self.masks_dir, self.batch_size)\n",
        "\n",
        "        while True:\n",
        "            x1i = next(image_generator)\n",
        "            x2i = next(mask_generator)\n",
        "            x2i = legend.transform(\n",
        "                {0: 0, 1: 0, 2: 0, 3: 1, 4: 0, 5: 0, 6: 0, 7: 1, 8: 1}, x2i)\n",
        "            # x2i = legend.transform(\n",
        "            #     {0: 0, 1: 1, 2: 1, 3: 2, 4: 3, 5: 1, 6: 4, 7: 2, 8: 2}, x2i)  # 3 plus aggr\n",
        "            # x2i = legend.transform(\n",
        "            #     {0: 0, 1: 1, 2: 1, 3: 2, 4: 3, 5: 1, 6: 2, 7: 2, 8: 2}, x2i)  # 3 classes\n",
        "\n",
        "            if self.perform_onehot_encoding is True:\n",
        "                # one hot encode masks\n",
        "                x2i = [\n",
        "                    self.onehot_encode(x2i[x, :, :, :]) for x in\n",
        "                    range(x2i.shape[0])]\n",
        "\n",
        "            yield x1i, np.asarray(x2i)\n",
        "\n",
        "    def numpy_generator(self, data_dir, batch_size=5):\n",
        "        \"\"\"Generate batches of images.\n",
        "\n",
        "        :param data_dir: path to the directory containing images\n",
        "        :param batch_size: the number of samples that will be propagated\n",
        "            through the network at once\n",
        "        :return: yielded batch-sized np stack of images\n",
        "        \"\"\"\n",
        "        # list of files from which the batches will be created\n",
        "        source_list = sorted(os.listdir(data_dir))\n",
        "\n",
        "        index = 1\n",
        "        batch = []\n",
        "\n",
        "        while True:\n",
        "            for source in source_list:\n",
        "                image = self.transpose_image(data_dir, source)\n",
        "\n",
        "                # add the image to the batch\n",
        "                batch.append(image)\n",
        "\n",
        "                if index % batch_size == 0:\n",
        "                    # batch created, return it\n",
        "                    yield np.stack(batch)\n",
        "                    batch = []\n",
        "\n",
        "                index += 1\n",
        "\n",
        "    def get_transposed_images(self, data_dir):\n",
        "        \"\"\"Get a list of transposed images.\n",
        "\n",
        "        :param data_dir: path to the directory containing images\n",
        "        :return: list of transposed numpy matrices representing images in\n",
        "            the dataset\n",
        "        \"\"\"\n",
        "        # list of files from which the dataset will be created\n",
        "        files_list = sorted(os.listdir(data_dir))\n",
        "\n",
        "        images_list = [\n",
        "            self.transpose_image(data_dir, file) for file in\n",
        "            files_list]\n",
        "\n",
        "        return images_list\n",
        "\n",
        "    @staticmethod\n",
        "    def transpose_image(data_dir, image_name):\n",
        "        \"\"\"Open an image and transpose it to (1, 2, 0).\n",
        "\n",
        "        :param data_dir: path to the directory containing images\n",
        "        :param image_name: name of the image file in the data dir\n",
        "        :return: the transposed image as a numpy array\n",
        "        \"\"\"\n",
        "        image = gdal.Open(os.path.join(data_dir, image_name), gdal.GA_ReadOnly)\n",
        "        image_array = image.ReadAsArray()\n",
        "\n",
        "        # GDAL reads masks as having no third dimension\n",
        "        # (we want it to be equal to one)\n",
        "        if image_array.ndim == 2:\n",
        "            transposed = np.expand_dims(image_array, -1)\n",
        "        else:\n",
        "            # move the batch to be the last dimension\n",
        "            transposed = np.moveaxis(image.ReadAsArray(), 0, -1)\n",
        "\n",
        "        image = None\n",
        "\n",
        "        return transposed\n",
        "\n",
        "    @staticmethod\n",
        "    def onehot_encode(orig_image):\n",
        "        \"\"\"Encode input images into one hot ones.\n",
        "\n",
        "        Unfortunately, keras.utils.to_categorical cannot be used because our\n",
        "        classes are not consecutive.\n",
        "\n",
        "        :param orig_image: original image\n",
        "            (height x width x num_classes)\n",
        "        \"\"\"\n",
        "        unique_vals = np.unique(orig_image)\n",
        "        num_classes = len(unique_vals)\n",
        "        shape = orig_image.shape[:2] + (num_classes,)\n",
        "        encoded_image = np.empty(shape, dtype=np.uint8)\n",
        "\n",
        "        # reshape to the shape used inside the onehot matrix\n",
        "        reshaped = orig_image.reshape((-1, 1))\n",
        "\n",
        "        for i, _ in enumerate(np.unique(reshaped)):\n",
        "            all_ax = np.all(reshaped == i, axis=1)\n",
        "            encoded_image[:, :, i] = all_ax.reshape(shape[:2])\n",
        "\n",
        "        return encoded_image\n",
        "\n",
        "\n",
        "def onehot_decode(onehot, nr_bands=3, enhance_colours=True):\n",
        "    \"\"\"Decode onehot mask labels to an eye-readable image.\n",
        "\n",
        "    :param onehot: one hot encoded image matrix (height x width x\n",
        "        num_classes)\n",
        "    :param colormap: dictionary mapping label ids to their codes\n",
        "    :param nr_bands: number of bands of intended input images\n",
        "    :param enhance_colours: Enhance the contrast between colours\n",
        "        (pseudorandom multiplication of the colour value)\n",
        "    :return: decoded RGB image (height x width x 3)\n",
        "    \"\"\"\n",
        "    # create 2D matrix with label ids (so you do not have to loop)\n",
        "    single_layer = np.argmax(onehot, axis=-1)\n",
        "\n",
        "    # create colourful visualizations\n",
        "    out_shape = (onehot.shape[0], onehot.shape[1], nr_bands)\n",
        "    output = np.zeros(out_shape)\n",
        "    for k in range(onehot.shape[2]):\n",
        "        output[single_layer == k] = k\n",
        "\n",
        "    if enhance_colours is True:\n",
        "        multiply_vector = [i ** 3 for i in range(1, nr_bands + 1)]\n",
        "        enhancement_matrix = np.ones(out_shape) * np.array(multiply_vector,\n",
        "                                                           dtype=np.uint8)\n",
        "        output *= enhancement_matrix\n",
        "\n",
        "    return np.uint8(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tavQ2-MJ0x9E"
      },
      "source": [
        "Let's have a `train` and a `val` (validation) generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZptv5IV6tr9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the data. Just to check if it works."
      ],
      "metadata": {
        "id": "KhVK0lBvjN15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnuNW20FeupL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That looks actually terrible - but reality is for losers. Let's open the gates of perception and enhance the brightness."
      ],
      "metadata": {
        "id": "WMxpbiyOk4ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how to increase brightness?\n",
        "# -> put \"python increase brightness\" to Google\n",
        "# copy the first solution on stackoverflow: https://stackoverflow.com/questions/32609098/how-to-fast-change-image-brightness-with-python-opencv\n",
        "def increase_brightness(img, value=30):\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    h, s, v = cv2.split(hsv)\n",
        "\n",
        "    lim = 255 - value\n",
        "    v[v > lim] = 255\n",
        "    v[v <= lim] += value\n",
        "\n",
        "    final_hsv = cv2.merge((h, s, v))\n",
        "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
        "    return img"
      ],
      "metadata": {
        "id": "4seYuTzxlJBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nSr_Fu20lL3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raXLOfBk1pLV"
      },
      "source": [
        "# Metrics\n",
        "\n",
        "We are going to write our own metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJLriYXX1oZU"
      },
      "outputs": [],
      "source": [
        "def mean_iou(y_true, y_pred):\n",
        "    yt0 = y_true[:,:,:,0]\n",
        "    yp0 = K.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n",
        "    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
        "    union = tf.math.count_nonzero(tf.add(yt0, yp0))\n",
        "    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
        "    return iou\n",
        "\n",
        "def categorical_dice(ground_truth_onehot, predictions, weights=1):\n",
        "    \"\"\"Compute the Sorensen-Dice loss.\n",
        "\n",
        "    :param ground_truth_onehot: onehot ground truth labels\n",
        "        (batch_size, img_height, img_width, nr_classes)\n",
        "    :param predictions: predictions from the last layer of the CNN\n",
        "        (batch_size, img_height, img_width, nr_classes)\n",
        "    :param weights: weights for individual classes\n",
        "        (number-of-classes-long vector)\n",
        "    :return: dice loss value averaged for all classes\n",
        "    \"\"\"\n",
        "    loss = categorical_tversky(ground_truth_onehot, predictions, 0.5, 0.5,\n",
        "                               weights)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def categorical_tversky(ground_truth_onehot, predictions, alpha=0.5,\n",
        "                        beta=0.5, weights=1):\n",
        "    \"\"\"Compute the Tversky loss.\n",
        "\n",
        "    alpha == beta == 0.5 -> Dice loss\n",
        "    alpha == beta == 1 -> Tanimoto coefficient/loss\n",
        "\n",
        "    :param ground_truth_onehot: onehot ground truth labels\n",
        "        (batch_size, img_height, img_width, nr_classes)\n",
        "    :param predictions: predictions from the last layer of the CNN\n",
        "        (batch_size, img_height, img_width, nr_classes)\n",
        "    :param alpha: magnitude of penalties for false positives\n",
        "    :param beta: magnitude of penalties for false negatives\n",
        "    :param weights: weights for individual classes\n",
        "        (number-of-classes-long vector)\n",
        "    :return: dice loss value averaged for all classes\n",
        "    \"\"\"\n",
        "    weight_tensor = tf.constant(weights, dtype=tf.float32)\n",
        "    predictions = tf.cast(predictions, tf.float32)\n",
        "    ground_truth_onehot = tf.cast(ground_truth_onehot, tf.float32)\n",
        "\n",
        "    # compute true positives, false negatives and false positives\n",
        "    true_pos = ground_truth_onehot * predictions\n",
        "    false_neg = ground_truth_onehot * (1. - predictions)\n",
        "    false_pos = (1. - ground_truth_onehot) * predictions\n",
        "\n",
        "    # compute Tversky coefficient\n",
        "    numerator = true_pos\n",
        "    numerator = tf.reduce_sum(numerator, axis=(1, 2))\n",
        "    denominator = true_pos + alpha * false_neg + beta * false_pos\n",
        "    denominator = tf.reduce_sum(denominator, axis=(1, 2))\n",
        "    tversky = numerator / denominator\n",
        "\n",
        "    # reduce mean for batches\n",
        "    tversky = tf.reduce_mean(tversky, axis=0)\n",
        "\n",
        "    # reduce mean for classes and multiply them by weights\n",
        "    loss = 1 - tf.reduce_mean(weight_tensor * tversky)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "160g6Ex41r-2"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write our own U-Net. A simple way."
      ],
      "metadata": {
        "id": "14p0Ec7slkMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hONrrUbW9CM_"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(437294792)\n",
        "# tf.random.set_seed(2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67Fyeczk_zzh"
      },
      "outputs": [],
      "source": [
        "model = unet()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look what the model looks like"
      ],
      "metadata": {
        "id": "JWajBDtxTCnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "nNdWeY9dTFno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU6SPuVY8Mdc"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's start the fun."
      ],
      "metadata": {
        "id": "rS59SqMBl4Et"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MXGinNg9Wjj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7QY8rgO1zUU"
      },
      "source": [
        "# Callbacks\n",
        "\n",
        "Fancy stuff to do at the end of each epoch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jZoIXhrcWI2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O5zCKBr8OZ1"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S17rew7Vqk5e"
      },
      "outputs": [],
      "source": [
        "x, y = next(val_generator)\n",
        "        \n",
        "# get the prediction\n",
        "pred = model.predict(x)\n",
        "\n",
        "# prediction post-processing for the sake of visualization\n",
        "pred_squeezed  = pred.squeeze()\n",
        "# normalize the image and make it an integer one\n",
        "img = (x[0][:, :, :3] * 255 / np.max(x[0][:, :, :3])).astype(np.uint8)\n",
        "img = increase_brightness(img, value=60)\n",
        "y_decode = onehot_decode(y[0])\n",
        "mask = onehot_decode(pred_squeezed)\n",
        "\n",
        "#show the mask and the prediction\n",
        "combined = np.concatenate([img, y_decode * 8, mask * 8], axis=1)\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.axis('off')\n",
        "plt.imshow(combined)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "unet_odse_prague_2022_empty.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}